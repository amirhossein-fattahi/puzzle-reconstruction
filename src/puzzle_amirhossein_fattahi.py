# -*- coding: utf-8 -*-
"""Puzzle_Amirhossein_Fattahi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yUQnLzzRmM9s2UQoT4VqczN_ltE7HTs1
"""


import cv2
import numpy as np
import matplotlib.pyplot as plt
#from google.colab import files

# the function to display an image using matplotlib
def show_image(img, title="Image", cmap=None):
    plt.figure(figsize=(10, 10))
    plt.imshow(img, cmap=cmap)
    plt.title(title)
    plt.axis("off")
    plt.show()

# reference image and puzzle pieces
def load_images():
    reference_path = "./figs/peppa.png"  
    patches_path = ["./figs/pieces1.png", "./figs/pieces2.png", "./figs/pz.png"] 

    reference_img = cv2.imread(reference_path, cv2.IMREAD_COLOR)
    patches = [cv2.imread(patch, cv2.IMREAD_COLOR) for patch in patches_path]

    return reference_img, patches

reference_img, patches = load_images()
show_image(reference_img, "Reference Image")
for i, patch in enumerate(patches):
    show_image(patch, f"Puzzle Piece {i+1}")

# default contour_area_ratio=0.01, morph_kernel_size=3
def segment_puzzle_pieces(image_path, contour_area_ratio=0.01, threshold_method="global", morph_kernel_size=3):
    image = cv2.imread(image_path)
    if image is None:
        print(f"Error: Unable to load image from {image_path}")
        return []

    # converting image to grayscale and reducing noise
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)


    # thresholding method
    if threshold_method == "global":
        _, binary = cv2.threshold(blurred, 128, 255, cv2.THRESH_BINARY_INV)
    elif threshold_method == "otsu":
        binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)
    else:
        raise ValueError("Unsupported threshold_method. Use 'global' or 'otsu'.")

    # operations to close gaps
    #kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (morph_kernel_size, morph_kernel_size))
    #closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (morph_kernel_size, morph_kernel_size))
    closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)


    # contours in the binary image
    contours, _ = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # acceptable contours based on area (min area)
    image_area = gray.shape[0] * gray.shape[1]
    min_contour_area = contour_area_ratio * image_area
    valid_contours = [c for c in contours if cv2.contourArea(c) > min_contour_area]

    print(f"Found {len(valid_contours)} contours in {image_path.split('/')[-1]}.")

    # extract individual puzzle pieces
    puzzle_pieces = []
    piece_masks = []

    for contour in valid_contours:
        # binary mask for the current contour and extracting the piece
        mask = np.zeros_like(gray, dtype=np.uint8)
        cv2.drawContours(mask, [contour], -1, 255, thickness=cv2.FILLED)
        piece = cv2.bitwise_and(image, image, mask=mask)

        # bounding box of the contour
        x, y, w, h = cv2.boundingRect(contour)
        cropped_piece = piece[y:y+h, x:x+w]
        cropped_mask = mask[y:y+h, x:x+w]

        puzzle_pieces.append(cropped_piece)
        piece_masks.append(cropped_mask)

    # show the binary image and contours
    _, axes = plt.subplots(1, 2, figsize=(12, 6))
    axes[0].imshow(binary, cmap='gray')
    axes[0].set_title("Thresholded Image")
    axes[0].axis('off')

    contour_visual = image.copy()
    cv2.drawContours(contour_visual, valid_contours, -1, (0, 255, 0), 2)
    axes[1].imshow(cv2.cvtColor(contour_visual, cv2.COLOR_BGR2RGB))
    axes[1].set_title("Detected Contours")
    axes[1].axis('off')
    plt.tight_layout()
    plt.show()

    return puzzle_pieces, piece_masks

# paths to the images
puzzle_images = [
    './figs/pieces1.png',
    './figs/pieces2.png',
    './figs/pz.png'
]

# specific contour_area_ratio for each image
contour_area_ratios = {
    './figs/pieces1.png': 0.011,  
    './figs/pieces2.png': 0.01,  
    './figs/pz.png': 0.0055       # smaller pieces, lower ratio
}

# apply the segmentation function to each image
all_puzzle_pieces = []  
all_puzzle_masks = []
for image_path in puzzle_images:
    ratio = contour_area_ratios.get(image_path, 0.01)
    print(f"Processing {image_path} with contour_area_ratio={ratio}...")

    puzzle_pieces, puzzle_masks = segment_puzzle_pieces(image_path, contour_area_ratio=ratio, threshold_method="otsu", morph_kernel_size=5)
    all_puzzle_pieces.extend(puzzle_pieces)
    all_puzzle_masks.extend(puzzle_masks)

print(f"Total segmented puzzle pieces: {len(all_puzzle_pieces)}")

def extract_sift_features(image):
    # convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # initialize SIFT detector
    sift = cv2.SIFT_create()

    # compute keypoints and descriptors
    keypoints, descriptors = sift.detectAndCompute(gray, None)

    print(f"Extracted {len(keypoints)} keypoints.")
    return keypoints, descriptors

# reference image
reference_image_path = './figs/peppa.png'
reference_image = cv2.imread(reference_image_path)

if reference_image is None:
    print(f"Error: Could not load reference image at {reference_image_path}")
else:
    print("Processing reference image...")
    ref_keypoints, ref_descriptors = extract_sift_features(reference_image)
    print(f"Reference image: Extracted {len(ref_keypoints)} keypoints.")

# SIFT features for all segmented pieces
puzzle_features = [] 

for i, piece in enumerate(all_puzzle_pieces):
    print(f"Processing puzzle piece {i + 1}...")
    keypoints, descriptors = extract_sift_features(piece)
    puzzle_features.append((keypoints, descriptors))
    print(f"Puzzle piece {i + 1}: Extracted {len(keypoints)} keypoints.")

def match_features(ref_descriptors, piece_descriptors, ratio=0.6):
    # BFMatcher
    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)  # Use L2 norm for SIFT descriptors

    # knn matching
    matches = bf.knnMatch(ref_descriptors, piece_descriptors, k=2)  # Returns list of lists

    # apply the ratio test
    good_matches = []
    for m, n in matches:
        if m.distance < ratio * n.distance:
            good_matches.append(m)

    print(f"Number of good matches: {len(good_matches)}")
    return good_matches

matches_for_pieces = []

for i, (keypoints, descriptors) in enumerate(puzzle_features):
    print(f"Matching features for puzzle piece {i + 1}...")
    if descriptors is not None and ref_descriptors is not None:
        good_matches = match_features(ref_descriptors, descriptors, ratio=0.4)
        matches_for_pieces.append((i, good_matches))
    else:
        print(f"No descriptors found for puzzle piece {i + 1}. Skipping.")

def visualize_matches(reference_image, ref_keypoints, piece_image, piece_keypoints, matches):
    # draw matches line
    match_image = cv2.drawMatches(
        reference_image, ref_keypoints,
        piece_image, piece_keypoints,
        matches, None,
        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS
    )

    # show the image
    plt.figure(figsize=(15, 15))
    plt.imshow(cv2.cvtColor(match_image, cv2.COLOR_BGR2RGB))
    plt.title("Feature Matches")
    plt.axis("off")
    plt.show()

# EX: Visualize matches for one puzzle piece
if matches_for_pieces[0][1]: 
    piece_index, good_matches = matches_for_pieces[31]
    visualize_matches(reference_image, ref_keypoints, all_puzzle_pieces[piece_index],
                       puzzle_features[piece_index][0], good_matches)

def compute_homography(matches, ref_keypoints, piece_keypoints):
    # coordinates of matched keypoints
    ref_pts = np.float32([ref_keypoints[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
    piece_pts = np.float32([piece_keypoints[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)

    # the homography using RANSAC
    H, mask = cv2.findHomography(piece_pts, ref_pts, cv2.RANSAC, 5.0)

    print(f"Computed homography:\n{H}")
    print(f"Inliers: {np.sum(mask)} out of {len(matches)} matches")
    return H, mask

def warp_piece(piece_image, reference_image, H):
    # convert the piece to grayscale
    gray_piece = cv2.cvtColor(piece_image, cv2.COLOR_BGR2GRAY)

    # threshold to isolate the non-background region
    _, thresh = cv2.threshold(gray_piece, 1, 255, cv2.THRESH_BINARY)

    # contours in the binary image
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # assume the largest contour corresponds to the puzzle piece
    largest_contour = max(contours, key=cv2.contourArea)

    # a mask for the piece
    mask = np.zeros_like(gray_piece)
    cv2.drawContours(mask, [largest_contour], -1, 255, thickness=cv2.FILLED)

    # warp both the piece and its mask
    height, width, _ = reference_image.shape
    warped_piece = cv2.warpPerspective(
        piece_image, H, (width, height), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0)
    )
    warped_mask = cv2.warpPerspective(
        mask, H, (width, height), borderMode=cv2.BORDER_CONSTANT, borderValue=0
    )

    # extract the warped piece using the warped mask
    warped_piece = cv2.bitwise_and(warped_piece, warped_piece, mask=warped_mask)

    return warped_piece

def overlay_piece(reference_image, warped_piece):
    overlay = reference_image.copy()
    mask = (warped_piece > 0)
    overlay[mask] = warped_piece[mask]
    return overlay

#### EX: for a single puzzle piece
piece_index = 4  
good_matches = matches_for_pieces[piece_index][1]  
piece_image = all_puzzle_pieces[piece_index]
piece_keypoints, piece_descriptors = puzzle_features[piece_index]

# Homography
H, mask = compute_homography(good_matches, ref_keypoints, piece_keypoints)

# warp the piece
warped_piece = warp_piece(piece_image, reference_image, H)

# overlay the warped piece on the ref image
result = overlay_piece(reference_image, warped_piece)

# visualize the result
plt.figure(figsize=(10, 10))
plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))
plt.title("Overlayed Puzzle Piece")
plt.axis("off")
plt.show()

#  !!! Commenting due to long runtime (doing previous section for all 40 pieces) !!!
"""
# copy of the reference image to overlay all pieces
assembled_puzzle = reference_image.copy()

# for all puzzle pieces
for piece_index in range(len(all_puzzle_pieces)):
    print(f"Processing piece {piece_index + 1}...")

    good_matches = matches_for_pieces[piece_index][1]  # Matches for the current piece
    piece_image = all_puzzle_pieces[piece_index]
    piece_keypoints, piece_descriptors = puzzle_features[piece_index]

    # check if there are enough matches
    if len(good_matches) < 4:
        print(f"Skipping piece {piece_index + 1} due to insufficient matches ({len(good_matches)} matches).")
        continue

    # homography
    H, mask = compute_homography(good_matches, ref_keypoints, piece_keypoints)
    if H is None:
        print(f"Skipping piece {piece_index + 1} due to failed homography.")
        continue

    # warp the puzzle piece
    warped_piece = warp_piece(piece_image, reference_image, H)

    # overlay the warped piece on the reference image
    assembled_puzzle = overlay_piece(reference_image, warped_piece)

    # visualize the result
    plt.figure(figsize=(10, 10))
    plt.imshow(cv2.cvtColor(assembled_puzzle, cv2.COLOR_BGR2RGB))
    plt.title("Overlayed Puzzle Piece")
    plt.axis("off")
    plt.show()

# visualize the final assembled puzzle
#plt.figure(figsize=(10, 10))
#plt.imshow(cv2.cvtColor(assembled_puzzle, cv2.COLOR_BGR2RGB))
#plt.title("Fully Assembled Puzzle")
#plt.axis("off")
#plt.show()
"""

def overlay_all_pieces(reference_image, all_pieces, all_features, all_matches, ref_keypoints):
    assembled_image = reference_image.copy()

    for piece_index, (piece_image, (piece_keypoints, _), matches) in enumerate(zip(all_pieces, all_features, all_matches)):
        print(f"Processing piece {piece_index + 1}...")

        # Compute homography
        if matches:
            H, mask = compute_homography(matches, ref_keypoints, piece_keypoints)
            if H is None:
                print(f"Skipping piece {piece_index + 1} due to failed homography.")
                continue

            # warp the piece
            warped_piece = warp_piece(piece_image, reference_image, H)

            # overlay the warped piece
            assembled_image = overlay_piece(assembled_image, warped_piece)
        else:
            print(f"Skipping piece {piece_index + 1} due to insufficient matches.")

    return assembled_image

# all puzzle pieces and features by their respective images (1, 2, 3)
pic1_pieces = all_puzzle_pieces[:10]
pic2_pieces = all_puzzle_pieces[10:20]
pic3_pieces = all_puzzle_pieces[20:]

pic1_features = puzzle_features[:10]
pic2_features = puzzle_features[10:20]
pic3_features = puzzle_features[20:]

pic1_matches = [m[1] for m in matches_for_pieces[:10]]
pic2_matches = [m[1] for m in matches_for_pieces[10:20]]
pic3_matches = [m[1] for m in matches_for_pieces[20:]]

# overlay pieces from pic1
assembled_pic1 = overlay_all_pieces(reference_image, pic1_pieces, pic1_features, pic1_matches, ref_keypoints)
plt.figure(figsize=(10, 10))
plt.imshow(cv2.cvtColor(assembled_pic1, cv2.COLOR_BGR2RGB))
plt.title("Assembled Puzzle (pic1)")
plt.axis("off")
plt.show()

# overlay pieces from pic2
assembled_pic2 = overlay_all_pieces(reference_image, pic2_pieces, pic2_features, pic2_matches, ref_keypoints)
plt.figure(figsize=(10, 10))
plt.imshow(cv2.cvtColor(assembled_pic2, cv2.COLOR_BGR2RGB))
plt.title("Assembled Puzzle (pic2)")
plt.axis("off")
plt.show()

# overlay pieces from pic3
assembled_pic3 = overlay_all_pieces(reference_image, pic3_pieces, pic3_features, pic3_matches, ref_keypoints)
plt.figure(figsize=(10, 10))
plt.imshow(cv2.cvtColor(assembled_pic3, cv2.COLOR_BGR2RGB))
plt.title("Assembled Puzzle (pic3)")
plt.axis("off")
plt.show()

